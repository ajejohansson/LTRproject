{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import statistics\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from random import shuffle\n",
    "\n",
    "# Not runnable as of submission: requires data. See readme for content description. Contact me if anything is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dwug_data(target_words=tuple(), target_pos=tuple()):\n",
    "    '''\n",
    "    returns worddata from dwug annotations\n",
    "    args:\n",
    "        target_words: iterable of words to return. If empty, returns all words\n",
    "        target_pos: iterable of parts of speech to filter for. If empty, does not filter for part of speech\n",
    "    '''\n",
    "    \n",
    "    dwug_datapath = './annotation_data/dwug_en/data/'\n",
    "    worddata = {}\n",
    "    #worddata['skipped_words'] = []\n",
    "    skipped_words = []\n",
    "    for word in os.listdir(dwug_datapath):\n",
    "        if target_words:\n",
    "            if not word in target_words:\n",
    "                continue\n",
    "        if target_pos:\n",
    "            if not word.split('_')[1] in target_pos:\n",
    "                continue\n",
    "        wordpath = os.path.join(dwug_datapath, word)\n",
    "        #print(word)\n",
    "        try:\n",
    "            # The files for some words do not get read properly by pandas, I think because they contain mid-string apostrophes.\n",
    "            # Could be fixed with preprocessing, but my purpose with this data is to select a few appropriate words\n",
    "            # with which to compare annotations, so I do not need all the words anyway.\n",
    "\n",
    "            # Could set read_csv(on_bad_lines=skip) for the same effect as below, but this way I can track what I'm skipping.\n",
    "            worddata[word] = {}\n",
    "            worddata[word]['judgments'] = pd.read_csv(os.path.join(wordpath, 'judgments.csv'), sep='\\t')\n",
    "            worddata[word]['uses'] = pd.read_csv(os.path.join(wordpath, 'uses.csv'), sep='\\t')\n",
    "        except:\n",
    "            print('Skipping', word)\n",
    "            worddata.pop(word)\n",
    "            skipped_words.append(word) # record of what has been skipped\n",
    "    return worddata, skipped_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uses_and_pairs(worddata, skipped_words, prespecified_words=False, specifiers={'min_judgments': 3, 'num_words': 2, 'num_usepairs': 25}):\n",
    "\n",
    "    '''\n",
    "    returns pandas dataframes of uses and usepairs/pre-annotated judgment files in DURel readable format\n",
    "    args:\n",
    "        worddata: output format of get_dwug_data\n",
    "        skipped_words: array-like of manually skipped lemmas\n",
    "        prespecified_words: array-like of specific words to get. If truthy, other specifiers except 'num_usepairs' will be ignored\n",
    "        specifiers:\n",
    "            min_judgments: only get words with at least this many annotations\n",
    "            num_words: return data for this many words, sorted to include those with the most annotations\n",
    "            num_usepairs: number of edges included in usepairs file\n",
    "        \n",
    "    '''\n",
    "    # Commented out but kept some code used only to familiarise myself with the data\n",
    "    \n",
    "    # Get counts of judgment pairs\n",
    "    # judgment pair = unique combination of uses that cooccur in the annotation data, where pair-order does not matter\n",
    "\n",
    "    if prespecified_words:\n",
    "        print(\"Getting prespecified words. The only specifier filter that will be applied is 'num_usepairs'.\")\n",
    "        for word in skipped_words:\n",
    "            if word in prespecified_words:\n",
    "                print(word, 'was skipped in the get_data step and will not be returned')\n",
    "    #groupingdict = {}\n",
    "    perword_jpair_count = {}\n",
    "    for word, usejudge in list(worddata.items()):\n",
    "        if prespecified_words:\n",
    "            if not word in prespecified_words:              \n",
    "                continue\n",
    "\n",
    "        judgments = usejudge['judgments']\n",
    "        judgment_pairs = {}\n",
    "        for i in range(len(judgments)): #could loop over judgments, but iterating over dataframes is sloooow\n",
    "            row = judgments.iloc[i]\n",
    "            id1 = row['identifier1']\n",
    "            id2 = row['identifier2']\n",
    "            identifiers = frozenset([id1, id2]) # set because order of pairs do not matter; frozenset to make it hashable\n",
    "\n",
    "            # Block used to determine if uses are paired with uses of the same period\n",
    "            # or only the other period:\n",
    "\n",
    "            #usedata = worddata[word]['uses']\n",
    "            #userow1 = usedata.loc[usedata['identifier'] == id1]\n",
    "            # userow2 = usedata.loc[usedata['identifier'] == id2]\n",
    "            # 'grouping' = the time period the use is associated with.\n",
    "\n",
    "            #grouping1 = userow1['grouping'].iloc[0]\n",
    "            #grouping2 = userow2['grouping'].iloc[0]\n",
    "            #groupingstr = \" \".join([str(grouping1), str(grouping2)])\n",
    "            #if groupingstr not in groupingdict:\n",
    "            #    groupingdict[groupingstr] = 0\n",
    "            #groupingdict[groupingstr] +=1\n",
    "            \n",
    "\n",
    "            if row['judgment'] == 0: # 0-judgments are categorically different from the rest;\n",
    "                                    # they indicate something like 'nonjudgment' (due to nonunderstanding etc.), not a similarity score.\n",
    "                                    # Thus, they should not be used for the average, and I will not validate against it\n",
    "                continue\n",
    "\n",
    "            if not identifiers in judgment_pairs:\n",
    "                #judgment_pairs[identifiers] = {'annotators': {}}\n",
    "                judgment_pairs[identifiers] = [0, 0, []] # [count, sum(judgment), judgment list]\n",
    "            #judgment_pairs[identifiers][row['annotator']] = row['judgment']\n",
    "            judgment_pairs[identifiers][0]+=1\n",
    "            judgment_pairs[identifiers][1]+=row['judgment']\n",
    "            judgment_pairs[identifiers][2].append(row['judgment'])\n",
    "\n",
    "        perword_jpair_count[word] = {k: v for k, v in sorted(judgment_pairs.items(), key=lambda item: item[1][0], reverse=True)}\n",
    "        \n",
    "    for word, jpairs in perword_jpair_count.items():\n",
    "        for jpair, countsumjudg in jpairs.items():\n",
    "            c, s, j = countsumjudg\n",
    "            avrg = s/c\n",
    "            median = statistics.median(j)\n",
    "            perword_jpair_count[word][jpair] = [c, avrg, median]\n",
    "            \n",
    "\n",
    "\n",
    "    #print(groupingdict)\n",
    "    # outcome: {'1 1': 16089, '2 1': 12736, '2 2': 16967, '1 2': 18727}\n",
    "    # Mixed pairing, which is expected from the paper, but this confirms the approach I will use for my sampling\n",
    "\n",
    "    if not prespecified_words:\n",
    "        filtered_perword_jpair_count = {} # \n",
    "        for word, fsets in list(perword_jpair_count.items()):\n",
    "            filtered_perword_jpair_count[word] = {}\n",
    "\n",
    "            filtered_perword_jpair_count[word] = {fset: count_avg for fset, count_avg in fsets.items() if count_avg[0] >= specifiers['min_judgments']}\n",
    "\n",
    "        sorted_filtered = {k: v for k, v in sorted(filtered_perword_jpair_count.items(), key=lambda item: len(item[1]), reverse=True)}\n",
    "    \n",
    "        try:\n",
    "            selected_words = list(sorted_filtered.items())[:specifiers['num_words']]\n",
    "        except:\n",
    "            print('Current filters find fewer than specified {} words. Returning all words.'.format(specifiers['num_words']))\n",
    "            selected_words = list(sorted_filtered.items())\n",
    "    else:\n",
    "        selected_words = list(perword_jpair_count.items())\n",
    "\n",
    "\n",
    "    final_pairs = {}  \n",
    "    uses = {}\n",
    "    for word, fsets in selected_words:\n",
    "        try:\n",
    "            selected_pairs = list(fsets.items())[:specifiers['num_usepairs']]\n",
    "        except:\n",
    "            print(\"Current filters find fewer than specified {} usepairs for word '{}'. Returning all usepairs.\".format(specifiers['num_usepairs'], word))\n",
    "            selected_pairs = list(fsets.items())\n",
    "        final_pairs[word] = {fset: countavg for fset, countavg in selected_pairs}\n",
    "        \n",
    "        uses[word] = set()\n",
    "        for fset in selected_pairs:\n",
    "\n",
    "            uses[word].update(fset[0])\n",
    "\n",
    "    usepair_dfs = {} #final pair format\n",
    "\n",
    "    # Reformat uses\n",
    "    for word, fsets in final_pairs.items():\n",
    "\n",
    "        pandas_durel_format = {}\n",
    "        lemmas = []\n",
    "        id1s = []\n",
    "        id2s = []\n",
    "        avg_judgs = []\n",
    "        medians = []\n",
    "        for fset, metrics in fsets.items():\n",
    "            lemmas.append(word)\n",
    "            id1s.append(list(fset)[0])\n",
    "            id2s.append(list(fset)[1])\n",
    "            avg_judgs.append(metrics[1])\n",
    "            medians.append(metrics[2])     \n",
    "        pandas_durel_format['lemma'] = lemmas\n",
    "        pandas_durel_format['identifier1'] = id1s\n",
    "        pandas_durel_format['identifier2'] = id2s\n",
    "        pandas_durel_format['avg_judgment'] = avg_judgs\n",
    "        pandas_durel_format['median_judgment'] = medians\n",
    "\n",
    "\n",
    "        usepair_dfs[word] = pd.DataFrame.from_dict(pandas_durel_format)\n",
    "\n",
    "    \n",
    "\n",
    "    use_dfs = {} # final use format\n",
    "    for word, fsets in final_pairs.items(): # \n",
    "        usedata = worddata[word]['uses']\n",
    "\n",
    "        filtered_df = usedata.loc[usedata['identifier'].isin(uses[word])] # filter original dataframe to only include uses from among my selected usepairs\n",
    "        filtered_df = filtered_df.reset_index(drop=True)\n",
    "        filtered_df = filtered_df.drop(columns=['context_tokenized', 'indexes_target_token_tokenized', 'indexes_target_sentence_tokenized', 'context_lemmatized', 'context_pos'])\n",
    "\n",
    "        use_dfs[word] = filtered_df\n",
    "\n",
    "    return usepair_dfs, use_dfs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(word_dict, return_vocab=False):\n",
    "    '''\n",
    "    Gets sentences from CCOHA sample\n",
    "    word_dict: dict where keys = lemmas and values = {'forms': (word forms to match,), 'tags': (NLTK tags to match,)}\n",
    "    values of keys 'forms', 'tags' must be array-like even if only singleton member. 'lemma' can be more than just the lemma,\n",
    "    (e.g. '[lemma]_nn' to differentiate lemmas of different parts of speech with the same form), in which case the dict should have key\n",
    "    'search_lemma' with value [lemma]\n",
    "    '''\n",
    "    \n",
    "    if return_vocab:\n",
    "        vocab = {}\n",
    "    sentence_ids = {}\n",
    "    sentences = {}\n",
    "    lemmas = word_dict.keys()\n",
    "    for lemma in lemmas:\n",
    "        sentence_ids[lemma] = {}\n",
    "        sentences[lemma] = {}\n",
    "\n",
    "    print('Finding lemma locations...')    \n",
    "    for corpus_i in range(1,3): # corpus_i = 1 or 2, the ids for the timeperiods\n",
    "        for lemma in lemmas:\n",
    "            sentence_ids[lemma][str(corpus_i)] = []\n",
    "        with open(os.path.join(os.getcwd(), 'semeval2020_ulscd_eng', 'corpus'+str(corpus_i), 'lemma', 'ccoha'+str(corpus_i)+'.txt')) as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            for i, line in enumerate(lines):\n",
    "                for lemma in lemmas:\n",
    "                    if 'search_lemma' in word_dict[lemma]:\n",
    "                        search_lemma = word_dict[lemma]['search_lemma']\n",
    "                    else:\n",
    "                        search_lemma = lemma\n",
    "                    \n",
    "                    if search_lemma in line:\n",
    "                        sentence_ids[lemma][str(corpus_i)].append(i)\n",
    "\n",
    "    c = 0\n",
    "    easy_puncts = [\".\", \",\", \"!\", \"?\", \":\", \";\"] # punctuation that is nearly universally caught by an align-with-lefthand-token heuristic\n",
    "                                        # obviously not ideal, but this is only for some slight readibility improvements and is not\n",
    "                                        # computationally necessary\n",
    "    used_identifiers = {}\n",
    "    for lemma in lemmas:\n",
    "        used_identifiers[lemma] = [0]\n",
    "        # want the identifier to be unique per lemma, not per lemma per corpus, but I'm also keeping the corpora separate for now,\n",
    "        # so this is the memory solution\n",
    "    print('Retrieving data for...')\n",
    "    for corpus_i in range(1, 3):\n",
    "        print('\\tCorpus group', corpus_i)\n",
    "        for lemma in lemmas:\n",
    "            sentences[lemma][str(corpus_i)] = []\n",
    "        with open(os.path.join(os.getcwd(), 'semeval2020_ulscd_eng', 'corpus'+str(corpus_i), 'token', 'ccoha'+str(corpus_i)+'.txt')) as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            for lemma in lemmas:\n",
    " \n",
    "                print('\\t\\t', lemma)\n",
    "                for id in sentence_ids[lemma][str(corpus_i)]:\n",
    "                    token_line = lines[id]\n",
    "                    toktagged = pos_tag(word_tokenize(token_line))\n",
    "                    toks = [tok for tok, tag in toktagged]\n",
    "                    tags = [tag for tok, tag in toktagged]\n",
    "\n",
    "                    punct_ids = {}\n",
    "                    for i in range(len(tags)):\n",
    "                        if toks[i] in easy_puncts:\n",
    "                            punct_ids[i] = toks[i]\n",
    "\n",
    "                    for i, tok in enumerate(toks):\n",
    "                        if tok.lower() in word_dict[lemma]['forms']: # if the word occurs multiple times, they will get separate entries with the same overall context\n",
    "                                                                    # (not exactly the same left/right context)\n",
    "                                                                    # this doesn't seem like a fundamental problem since they are separate uses, but if a lot of uses are from the\n",
    "                                                                    # same source, the most prevalently used sense might be overrepresented\n",
    "                            if tags[i] in word_dict[lemma]['tags'] or not word_dict[lemma]['tags']:\n",
    "                                if not word_dict[lemma]['tags']:\n",
    "                                    #input dict['tags'] can be empty, in which case word match is enough\n",
    "                                    #still needs to be in input dict as a falsy iterable, or there will be an error\n",
    "                                    pos = ''\n",
    "                                else:\n",
    "                                    pos = [tag for tag in word_dict[lemma]['tags'] if tag == tags[i]][0]\n",
    "                                \n",
    "                                grouping = str(corpus_i)\n",
    "                                identifier = used_identifiers[lemma][-1]+1\n",
    "                                used_identifiers[lemma].append(identifier)\n",
    "                                #context = lines[id] # technically better context, but complicates getting specific slices of character indices for durel\n",
    "\n",
    "                                target_len = len(tok)\n",
    "                                target_start = len(' '.join(toks[:i]))+1 \n",
    "                                \n",
    "                                # the following until -------------- is a more convoluted than technically necessary way of getting the context.\n",
    "                                # It is meant to produce contexts without uncessary spaces produced by tokenisation for at least some punctuation,\n",
    "                                # which is only for readability.\n",
    "                                # it loops over the line a lot and is thus (relatively) quite a bit more computationally heavy than an approach that takes\n",
    "                                # the extra spaces as they are. It's not too much computation for my purposes, but could be a problem for more data.\n",
    "                                target_movement = 0\n",
    "                                context = []\n",
    "                                #print('len toks',len(toks))\n",
    "                                for j in range(len(toks)):\n",
    "                                    if toks[j] in easy_puncts:\n",
    "                                        if j < i:\n",
    "                                            target_start -=1 # punctuation tokens being merged with previous tokens = \n",
    "                                                            # fewer spaces added when joining token lists = \n",
    "                                                            # target index moves\n",
    "                                        continue\n",
    "                                    k = 1\n",
    "                                    updated_token = toks[j]\n",
    "\n",
    "                                    if j+k < len(toks):                                    \n",
    "                                        while toks[j+k] in easy_puncts:\n",
    "                                            updated_token = updated_token+toks[j+k]\n",
    "                                            k+=1\n",
    "                                            if j+k >= len(toks):\n",
    "                                                break\n",
    "                                            \n",
    "                                    context.append(updated_token)\n",
    "\n",
    "\n",
    "                                new_i = i-target_movement\n",
    "\n",
    "                                context = ' '.join(context)\n",
    "                                # ----------------------------\n",
    "\n",
    "\n",
    "                                indexes_target_token = '{}:{}'.format(str(target_start), str(target_start+target_len))\n",
    "\n",
    "                                indexes_target_sentence = '0:{}'.format(len(context))\n",
    "                                                     \n",
    "                                # not sure if the empty fields are necessary for durel, but might as well include them since work down the line could want to fill them\n",
    "                                durel_dict = {'lemma': lemma,\n",
    "                                              'pos': pos,\n",
    "                                              'date': '', \n",
    "                                              'grouping': grouping,\n",
    "                                              'identifier': lemma+'_'+str(identifier),\n",
    "                                              'description': '',\n",
    "                                              'context': context,\n",
    "                                              'indexes_target_token': indexes_target_token,\n",
    "                                              'indexes_target_sentence': indexes_target_sentence\n",
    "                                              } # format for pandas/durel\n",
    "                                sentences[lemma][str(corpus_i)].append(durel_dict)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab():\n",
    "    vocab = {}\n",
    "    for corpus_i in range(1,3):\n",
    "        with open(os.path.join(os.getcwd(), 'semeval2020_ulscd_eng', 'corpus'+str(corpus_i), 'lemma', 'ccoha'+str(corpus_i)+'.txt')) as f:\n",
    "            words = f.read().split()\n",
    "            for word in words:\n",
    "                if not word in vocab:\n",
    "                    vocab[word] = {}\n",
    "                if str(corpus_i) not in vocab[word]:\n",
    "                    vocab[word][str(corpus_i)] = 0\n",
    "                vocab[word][str(corpus_i)] +=1\n",
    "    for word, groupcount in vocab.items():\n",
    "        for group in ('1', '2'):\n",
    "            if group not in groupcount:\n",
    "                vocab[word][group] = 0\n",
    "    return vocab    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_vocab(vocab, mincount=0, maxcount = float('inf'), slicer_pattern=None):\n",
    "    '''\n",
    "    vocab: vocab with structure as per output of get_vocab\n",
    "    mincount: return words with higher count than this in BOTH corpora\n",
    "    maxcount: return words with lower count than this in BOTH corpora\n",
    "    slicer_pattern: Either 2-place array-like (slice(parts of word to inspect), 'string pattern to match').\n",
    "                    E.g., (slice(-2, None), 'ed') checks if the last two letters of the word is 'ed' OR\n",
    "                    Falsy value, in which case no pattern-based filtering is done                    \n",
    "    '''\n",
    "    # mincount to get words with high enough sample, maxcount to filter out very common words that might not be sought\n",
    "    # slicer_pattern can also be just a string, in which case the only returned vocab key will be the one exactly matching that input.\n",
    "    # However, this is only minorly different from just calling the vocab with that string, so makes the function call a bit redundant\n",
    "\n",
    "    fil_vocab = {}\n",
    "    if slicer_pattern:\n",
    "        if type(slicer_pattern) == str: # lets slicer_pattern basically just \n",
    "            slicer = slice(0, None)\n",
    "            pattern = slicer_pattern\n",
    "        else:\n",
    "            slicer = slicer_pattern[0]\n",
    "            pattern = slicer_pattern[1]\n",
    "    for word, groupcount in vocab.items():\n",
    "        if mincount <= groupcount['1'] <= maxcount and mincount <= groupcount['2'] <= maxcount:\n",
    "            keep = True\n",
    "            if slicer_pattern:\n",
    "                if not word[slicer] == pattern:\n",
    "                    keep = False\n",
    "            if keep:\n",
    "                fil_vocab[word] = vocab[word]\n",
    "\n",
    "    vocab_items = list(fil_vocab.items())\n",
    "    value_sum_sorted = sorted(vocab_items, key=lambda x: sum(x[1].values()), reverse=True)\n",
    "    sorted_vocab = {word: values for word, values in value_sum_sorted}\n",
    "\n",
    "    return sorted_vocab\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vocab = filter_vocab(vocab, mincount = 5, slicer_pattern=(slice(-2, None), 'ed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'need': {'1': 1600, '2': 4590},\n",
       " 'united': {'1': 1584, '2': 2052},\n",
       " 'indeed': {'1': 2344, '2': 775},\n",
       " 'hundred': {'1': 1903, '2': 1030},\n",
       " 'bed': {'1': 1012, '2': 1785},\n",
       " 'red': {'1': 1004, '2': 1765},\n",
       " 'proceed': {'1': 1501, '2': 289},\n",
       " 'succeed': {'1': 827, '2': 322},\n",
       " 'speed': {'1': 391, '2': 693},\n",
       " 'feed': {'1': 344, '2': 666},\n",
       " 'supposed': {'1': 383, '2': 575},\n",
       " 'concerned': {'1': 233, '2': 513},\n",
       " 'interested': {'1': 236, '2': 509},\n",
       " 'sacred': {'1': 550, '2': 148},\n",
       " 'married': {'1': 239, '2': 423},\n",
       " 'deed': {'1': 571, '2': 86},\n",
       " 'shed': {'1': 397, '2': 197},\n",
       " 'tired': {'1': 185, '2': 393},\n",
       " 'seed': {'1': 322, '2': 240},\n",
       " 'distinguished': {'1': 440, '2': 95},\n",
       " 'pleased': {'1': 337, '2': 169},\n",
       " 'naked': {'1': 222, '2': 277},\n",
       " 'involved': {'1': 125, '2': 364},\n",
       " 'exceed': {'1': 323, '2': 132},\n",
       " 'beloved': {'1': 290, '2': 112},\n",
       " 'increased': {'1': 153, '2': 240},\n",
       " 'inclined': {'1': 280, '2': 104},\n",
       " 'limited': {'1': 141, '2': 237},\n",
       " 'ed': {'1': 68, '2': 301},\n",
       " 'armed': {'1': 124, '2': 232},\n",
       " 'wicked': {'1': 268, '2': 78},\n",
       " 'unexpected': {'1': 190, '2': 140},\n",
       " 'satisfied': {'1': 238, '2': 91},\n",
       " 'wretched': {'1': 277, '2': 40},\n",
       " 'civilized': {'1': 247, '2': 68},\n",
       " 'breed': {'1': 135, '2': 174},\n",
       " 'bleed': {'1': 165, '2': 140},\n",
       " 'ned': {'1': 244, '2': 58},\n",
       " 'fred': {'1': 31, '2': 270},\n",
       " 'aged': {'1': 218, '2': 75},\n",
       " 'ashamed': {'1': 173, '2': 108},\n",
       " 'disappointed': {'1': 171, '2': 94},\n",
       " 'weed': {'1': 111, '2': 148},\n",
       " 'hatred': {'1': 160, '2': 98},\n",
       " 'delighted': {'1': 184, '2': 62},\n",
       " 'crowded': {'1': 99, '2': 143},\n",
       " 'complicated': {'1': 58, '2': 174},\n",
       " 'heed': {'1': 177, '2': 53},\n",
       " 'so-called': {'1': 28, '2': 192},\n",
       " 'embarrassed': {'1': 56, '2': 151},\n",
       " 'kindred': {'1': 187, '2': 17},\n",
       " 'old-fashioned': {'1': 81, '2': 116},\n",
       " 'reed': {'1': 91, '2': 103},\n",
       " 'enlightened': {'1': 160, '2': 33},\n",
       " 'advanced': {'1': 74, '2': 117},\n",
       " 'steed': {'1': 154, '2': 36},\n",
       " 'elevated': {'1': 148, '2': 39},\n",
       " 'creed': {'1': 148, '2': 38},\n",
       " 'ragged': {'1': 97, '2': 86},\n",
       " 'dignified': {'1': 157, '2': 25},\n",
       " 'alfred': {'1': 84, '2': 98},\n",
       " 'polished': {'1': 110, '2': 71},\n",
       " 'animated': {'1': 132, '2': 45},\n",
       " 'scattered': {'1': 113, '2': 63},\n",
       " 'indebted': {'1': 158, '2': 18},\n",
       " 'rugged': {'1': 117, '2': 58},\n",
       " 'damned': {'1': 29, '2': 146},\n",
       " 'detailed': {'1': 40, '2': 128},\n",
       " 'refined': {'1': 138, '2': 24},\n",
       " 'varied': {'1': 112, '2': 47},\n",
       " 'bewildered': {'1': 109, '2': 47},\n",
       " 'gifted': {'1': 98, '2': 57},\n",
       " 'cultivated': {'1': 117, '2': 26},\n",
       " 'ted': {'1': 7, '2': 136},\n",
       " 'renewed': {'1': 83, '2': 54},\n",
       " 'dried': {'1': 40, '2': 95},\n",
       " 'accomplished': {'1': 111, '2': 23},\n",
       " 'subdued': {'1': 97, '2': 34},\n",
       " 'amused': {'1': 57, '2': 70},\n",
       " 'middle-aged': {'1': 22, '2': 105},\n",
       " 'unprecedented': {'1': 27, '2': 99},\n",
       " 'accustomed': {'1': 106, '2': 16},\n",
       " 'puzzled': {'1': 43, '2': 78},\n",
       " 'depressed': {'1': 35, '2': 85},\n",
       " 'wed': {'1': 80, '2': 38},\n",
       " 'deceased': {'1': 91, '2': 25},\n",
       " 'printed': {'1': 77, '2': 38},\n",
       " 'crooked': {'1': 57, '2': 57},\n",
       " 'skilled': {'1': 33, '2': 81},\n",
       " 'alleged': {'1': 25, '2': 88},\n",
       " 'exalted': {'1': 101, '2': 12},\n",
       " 'striped': {'1': 52, '2': 61},\n",
       " 'shred': {'1': 35, '2': 77},\n",
       " 'amazed': {'1': 49, '2': 63},\n",
       " 'unlimited': {'1': 55, '2': 56},\n",
       " 'deserted': {'1': 65, '2': 44},\n",
       " 'tangled': {'1': 66, '2': 42},\n",
       " 'alarmed': {'1': 77, '2': 26},\n",
       " 'allied': {'1': 38, '2': 63},\n",
       " 'departed': {'1': 86, '2': 14},\n",
       " 'privileged': {'1': 44, '2': 55},\n",
       " 'renowned': {'1': 66, '2': 32},\n",
       " 'folded': {'1': 45, '2': 52},\n",
       " 'gilded': {'1': 65, '2': 32},\n",
       " 'hallowed': {'1': 84, '2': 11},\n",
       " 'prolonged': {'1': 36, '2': 59},\n",
       " 'unfinished': {'1': 51, '2': 43},\n",
       " 'disinterested': {'1': 79, '2': 15},\n",
       " 'curved': {'1': 24, '2': 70},\n",
       " 'spirited': {'1': 65, '2': 27},\n",
       " 'fried': {'1': 10, '2': 81},\n",
       " 'accused': {'1': 58, '2': 32},\n",
       " 'talented': {'1': 11, '2': 79},\n",
       " 'shattered': {'1': 60, '2': 29},\n",
       " 'unchanged': {'1': 45, '2': 44},\n",
       " 'protracted': {'1': 67, '2': 21},\n",
       " 'favored': {'1': 60, '2': 28},\n",
       " 'darkened': {'1': 34, '2': 53},\n",
       " 'untouched': {'1': 39, '2': 45},\n",
       " 'wrinkled': {'1': 32, '2': 51},\n",
       " 'outstretched': {'1': 48, '2': 35},\n",
       " 'unmoved': {'1': 67, '2': 15},\n",
       " 'unnoticed': {'1': 53, '2': 29},\n",
       " 'uninterrupted': {'1': 61, '2': 20},\n",
       " 'secluded': {'1': 56, '2': 25},\n",
       " 'tattered': {'1': 46, '2': 34},\n",
       " 'dissatisfied': {'1': 56, '2': 23},\n",
       " 'wooded': {'1': 43, '2': 36},\n",
       " 'battered': {'1': 13, '2': 66},\n",
       " 'undisturbed': {'1': 51, '2': 27},\n",
       " 'famed': {'1': 31, '2': 47},\n",
       " 'stained': {'1': 27, '2': 50},\n",
       " 'rounded': {'1': 41, '2': 35},\n",
       " 'anticipated': {'1': 40, '2': 35},\n",
       " 'diseased': {'1': 56, '2': 19},\n",
       " 'mildred': {'1': 54, '2': 21},\n",
       " 'annoyed': {'1': 26, '2': 48},\n",
       " 'unemployed': {'1': 10, '2': 63},\n",
       " 'coloured': {'1': 61, '2': 11},\n",
       " 'good-natured': {'1': 60, '2': 12},\n",
       " 'accursed': {'1': 66, '2': 5},\n",
       " 'professed': {'1': 60, '2': 11},\n",
       " 'outraged': {'1': 27, '2': 43},\n",
       " 'disabled': {'1': 21, '2': 47},\n",
       " 'impassioned': {'1': 48, '2': 20},\n",
       " 'exaggerated': {'1': 35, '2': 32},\n",
       " 'inexperienced': {'1': 45, '2': 21},\n",
       " 'unaffected': {'1': 48, '2': 17},\n",
       " 'bloodshed': {'1': 48, '2': 17},\n",
       " 'imported': {'1': 24, '2': 40},\n",
       " 'disgusted': {'1': 37, '2': 27},\n",
       " 'marked': {'1': 34, '2': 30},\n",
       " 'sled': {'1': 23, '2': 41},\n",
       " 'baked': {'1': 19, '2': 45},\n",
       " 'bearded': {'1': 15, '2': 48},\n",
       " 'jagged': {'1': 19, '2': 44},\n",
       " 'unused': {'1': 26, '2': 36},\n",
       " 'cracked': {'1': 18, '2': 44},\n",
       " 'chopped': {'1': 6, '2': 56},\n",
       " 'concentrated': {'1': 29, '2': 32},\n",
       " 'preoccupied': {'1': 9, '2': 51},\n",
       " 'parched': {'1': 44, '2': 16},\n",
       " 'greed': {'1': 7, '2': 52},\n",
       " 'unqualified': {'1': 48, '2': 10},\n",
       " 'disordered': {'1': 50, '2': 8},\n",
       " 'powdered': {'1': 27, '2': 31},\n",
       " 'framed': {'1': 14, '2': 43},\n",
       " 'unfounded': {'1': 37, '2': 20},\n",
       " 'bruised': {'1': 34, '2': 23},\n",
       " 'accepted': {'1': 14, '2': 43},\n",
       " 'decayed': {'1': 49, '2': 7},\n",
       " 'unprepared': {'1': 31, '2': 24},\n",
       " 'unmarried': {'1': 21, '2': 33},\n",
       " 'unprotected': {'1': 41, '2': 13},\n",
       " 'enchanted': {'1': 41, '2': 13},\n",
       " 'distorted': {'1': 22, '2': 32},\n",
       " 'impoverished': {'1': 18, '2': 36},\n",
       " 'veiled': {'1': 22, '2': 32},\n",
       " 'embed': {'1': 5, '2': 49},\n",
       " 'consecrated': {'1': 46, '2': 7},\n",
       " 'dilapidated': {'1': 35, '2': 18},\n",
       " 'unarmed': {'1': 29, '2': 24},\n",
       " 'unrestrained': {'1': 43, '2': 9},\n",
       " 'flushed': {'1': 40, '2': 12},\n",
       " 'unaccustomed': {'1': 37, '2': 15},\n",
       " 'vanquished': {'1': 39, '2': 12},\n",
       " 'winged': {'1': 32, '2': 19},\n",
       " 'saturated': {'1': 10, '2': 41},\n",
       " 'assorted': {'1': 8, '2': 43},\n",
       " 'mangled': {'1': 35, '2': 15},\n",
       " 'tweed': {'1': 7, '2': 43},\n",
       " 'blackened': {'1': 28, '2': 22},\n",
       " 'unobserved': {'1': 44, '2': 5},\n",
       " 'recorded': {'1': 25, '2': 24},\n",
       " 'widowed': {'1': 29, '2': 20},\n",
       " 'boiled': {'1': 23, '2': 25},\n",
       " 'discontented': {'1': 42, '2': 6},\n",
       " 'pronounced': {'1': 13, '2': 35},\n",
       " 'sliced': {'1': 5, '2': 42},\n",
       " 'divided': {'1': 22, '2': 24},\n",
       " 'cramped': {'1': 8, '2': 38},\n",
       " 'unauthorized': {'1': 22, '2': 23},\n",
       " 'antiquated': {'1': 27, '2': 18},\n",
       " 'crushed': {'1': 21, '2': 24},\n",
       " 'reserved': {'1': 25, '2': 20},\n",
       " 'revised': {'1': 14, '2': 31},\n",
       " 'licensed': {'1': 15, '2': 29},\n",
       " 'disciplined': {'1': 21, '2': 23},\n",
       " 'misguided': {'1': 26, '2': 18},\n",
       " 'concerted': {'1': 27, '2': 17},\n",
       " 'undoubted': {'1': 39, '2': 5},\n",
       " 'elated': {'1': 29, '2': 14},\n",
       " 'bereaved': {'1': 31, '2': 12},\n",
       " 'deformed': {'1': 28, '2': 15},\n",
       " 'inverted': {'1': 24, '2': 19},\n",
       " 'imagined': {'1': 14, '2': 29},\n",
       " 'attempted': {'1': 12, '2': 30},\n",
       " 'unoccupied': {'1': 30, '2': 11},\n",
       " 'crippled': {'1': 14, '2': 27},\n",
       " 'unheeded': {'1': 34, '2': 7},\n",
       " 'reformed': {'1': 23, '2': 18},\n",
       " 'fabled': {'1': 24, '2': 17},\n",
       " 'barbed': {'1': 12, '2': 29},\n",
       " 'conquered': {'1': 30, '2': 11},\n",
       " 'sustained': {'1': 7, '2': 34},\n",
       " 'retarded': {'1': 10, '2': 31},\n",
       " 'restricted': {'1': 18, '2': 23},\n",
       " 'unsettled': {'1': 32, '2': 8},\n",
       " 'locked': {'1': 11, '2': 29},\n",
       " 'avowed': {'1': 31, '2': 9},\n",
       " 'famished': {'1': 30, '2': 10},\n",
       " 'bloated': {'1': 16, '2': 24},\n",
       " 'hooded': {'1': 10, '2': 30},\n",
       " 'undisputed': {'1': 26, '2': 14},\n",
       " 'disgraced': {'1': 28, '2': 11},\n",
       " 'feathered': {'1': 17, '2': 22},\n",
       " 'heightened': {'1': 18, '2': 21},\n",
       " 'thatched': {'1': 23, '2': 16},\n",
       " 'roasted': {'1': 16, '2': 23},\n",
       " 'devoted': {'1': 28, '2': 10},\n",
       " 'softened': {'1': 31, '2': 7},\n",
       " 'unparalleled': {'1': 27, '2': 11},\n",
       " 'mutilated': {'1': 22, '2': 16},\n",
       " 'estranged': {'1': 21, '2': 17},\n",
       " 'well-dressed': {'1': 17, '2': 21},\n",
       " 'fated': {'1': 28, '2': 10},\n",
       " 'dred': {'1': 32, '2': 5},\n",
       " 'benighted': {'1': 32, '2': 5},\n",
       " 'strained': {'1': 14, '2': 23},\n",
       " 'masked': {'1': 11, '2': 26},\n",
       " 'upturned': {'1': 21, '2': 16},\n",
       " 'mottled': {'1': 14, '2': 23},\n",
       " 'stunted': {'1': 22, '2': 15},\n",
       " 'contented': {'1': 28, '2': 9},\n",
       " 'esteemed': {'1': 25, '2': 12},\n",
       " 'sculptured': {'1': 30, '2': 7},\n",
       " 'unmarked': {'1': 14, '2': 23},\n",
       " 'uncontrolled': {'1': 18, '2': 18},\n",
       " 'mohammed': {'1': 6, '2': 30},\n",
       " 'detached': {'1': 20, '2': 16},\n",
       " 'hushed': {'1': 18, '2': 18},\n",
       " 'convicted': {'1': 11, '2': 25},\n",
       " 'seasoned': {'1': 14, '2': 22},\n",
       " 'unconcerned': {'1': 17, '2': 19},\n",
       " 'stuffed': {'1': 7, '2': 29},\n",
       " 'grated': {'1': 24, '2': 10},\n",
       " 'gnarled': {'1': 11, '2': 23},\n",
       " 'blue-eyed': {'1': 14, '2': 20},\n",
       " 'untutored': {'1': 28, '2': 6},\n",
       " 'fringed': {'1': 23, '2': 10},\n",
       " 'crazed': {'1': 17, '2': 16},\n",
       " 'gray-haired': {'1': 10, '2': 23},\n",
       " 'pained': {'1': 14, '2': 19},\n",
       " 'tanned': {'1': 5, '2': 28},\n",
       " 'undiminished': {'1': 26, '2': 6},\n",
       " 'unsupported': {'1': 26, '2': 6},\n",
       " 'appalled': {'1': 16, '2': 16},\n",
       " 'infatuated': {'1': 24, '2': 8},\n",
       " 'soiled': {'1': 12, '2': 20},\n",
       " 'beleaguered': {'1': 6, '2': 26},\n",
       " 'poisoned': {'1': 19, '2': 12},\n",
       " 'jaded': {'1': 23, '2': 8},\n",
       " 'condensed': {'1': 18, '2': 13},\n",
       " 'dejected': {'1': 22, '2': 9},\n",
       " 'unharmed': {'1': 20, '2': 11},\n",
       " 'cocked': {'1': 23, '2': 8},\n",
       " 'abashed': {'1': 26, '2': 5},\n",
       " 'observed': {'1': 17, '2': 13},\n",
       " 'short-lived': {'1': 15, '2': 15},\n",
       " 'charmed': {'1': 22, '2': 8},\n",
       " 'curled': {'1': 14, '2': 16},\n",
       " 'upraised': {'1': 21, '2': 9},\n",
       " 'unquestioned': {'1': 21, '2': 9},\n",
       " 'dark-haired': {'1': 5, '2': 25},\n",
       " 'waxed': {'1': 17, '2': 12},\n",
       " 'frenzied': {'1': 10, '2': 19},\n",
       " 'wedded': {'1': 21, '2': 8},\n",
       " 'exposed': {'1': 20, '2': 8},\n",
       " 'vexed': {'1': 23, '2': 5},\n",
       " 'undivided': {'1': 22, '2': 6},\n",
       " 'aggrieved': {'1': 14, '2': 14},\n",
       " 'bigoted': {'1': 22, '2': 6},\n",
       " 'chartered': {'1': 17, '2': 11},\n",
       " 'besieged': {'1': 19, '2': 8},\n",
       " 'unanswered': {'1': 7, '2': 20},\n",
       " 'unaided': {'1': 17, '2': 10},\n",
       " 'perfumed': {'1': 18, '2': 9},\n",
       " 'undaunted': {'1': 19, '2': 8},\n",
       " 'dark-eyed': {'1': 20, '2': 7},\n",
       " 'spotted': {'1': 15, '2': 12},\n",
       " 'white-haired': {'1': 9, '2': 18},\n",
       " 'unclouded': {'1': 22, '2': 5},\n",
       " 'uneducated': {'1': 15, '2': 12},\n",
       " 'unwarranted': {'1': 6, '2': 21},\n",
       " 'unsophisticated': {'1': 22, '2': 5},\n",
       " 'infected': {'1': 7, '2': 20},\n",
       " 'burnished': {'1': 18, '2': 8},\n",
       " 'ensconced': {'1': 15, '2': 11},\n",
       " 'domesticated': {'1': 19, '2': 7},\n",
       " 'revered': {'1': 16, '2': 10},\n",
       " 'unpublished': {'1': 9, '2': 17},\n",
       " 'undisguised': {'1': 20, '2': 6},\n",
       " 'chastened': {'1': 19, '2': 7},\n",
       " 'one-sided': {'1': 10, '2': 16},\n",
       " 'untamed': {'1': 21, '2': 5},\n",
       " 'unnamed': {'1': 6, '2': 20},\n",
       " 'soaked': {'1': 6, '2': 20},\n",
       " 'astounded': {'1': 15, '2': 10},\n",
       " 'elongated': {'1': 9, '2': 16},\n",
       " 'cold-blooded': {'1': 14, '2': 11},\n",
       " 'bottled': {'1': 5, '2': 20},\n",
       " 'unopened': {'1': 12, '2': 13},\n",
       " 'imbed': {'1': 18, '2': 7},\n",
       " 'prejudiced': {'1': 16, '2': 9},\n",
       " 'consolidated': {'1': 11, '2': 14},\n",
       " 'conceited': {'1': 19, '2': 6},\n",
       " 'rusted': {'1': 6, '2': 19},\n",
       " 'irritated': {'1': 11, '2': 14},\n",
       " 'discolored': {'1': 10, '2': 15},\n",
       " 'anguished': {'1': 7, '2': 18},\n",
       " 'red-faced': {'1': 8, '2': 17},\n",
       " 'corrugated': {'1': 5, '2': 20},\n",
       " 'plumed': {'1': 15, '2': 9},\n",
       " 'undiscovered': {'1': 16, '2': 8},\n",
       " 'unchecked': {'1': 13, '2': 11},\n",
       " 'frosted': {'1': 10, '2': 14},\n",
       " 'dimpled': {'1': 16, '2': 8},\n",
       " 'jared': {'1': 9, '2': 15},\n",
       " 'bronzed': {'1': 17, '2': 7},\n",
       " 'unbridled': {'1': 14, '2': 10},\n",
       " 'deferred': {'1': 13, '2': 11},\n",
       " 'uprooted': {'1': 9, '2': 15},\n",
       " 'completed': {'1': 5, '2': 19},\n",
       " 'iced': {'1': 6, '2': 18},\n",
       " 'unassisted': {'1': 15, '2': 8},\n",
       " 'scented': {'1': 11, '2': 12},\n",
       " 'black-eyed': {'1': 13, '2': 10},\n",
       " 'bright-eyed': {'1': 14, '2': 9},\n",
       " 'unrestricted': {'1': 11, '2': 12},\n",
       " 'blood-stained': {'1': 17, '2': 5},\n",
       " 'distilled': {'1': 15, '2': 7},\n",
       " 'unfettered': {'1': 14, '2': 8},\n",
       " 'disjointed': {'1': 17, '2': 5},\n",
       " 'unskilled': {'1': 11, '2': 11},\n",
       " 'disaffected': {'1': 17, '2': 5},\n",
       " 'half-naked': {'1': 10, '2': 12},\n",
       " 'salted': {'1': 8, '2': 14},\n",
       " 'speckled': {'1': 8, '2': 13},\n",
       " 'untroubled': {'1': 12, '2': 9},\n",
       " 'fevered': {'1': 10, '2': 11},\n",
       " 'hunted': {'1': 13, '2': 8},\n",
       " 'jed': {'1': 11, '2': 10},\n",
       " 'well-established': {'1': 8, '2': 13},\n",
       " 'reputed': {'1': 14, '2': 7},\n",
       " 'knotted': {'1': 11, '2': 10},\n",
       " 'emaciated': {'1': 15, '2': 6},\n",
       " 'tufted': {'1': 13, '2': 8},\n",
       " 'repressed': {'1': 12, '2': 9},\n",
       " 'unwashed': {'1': 6, '2': 15},\n",
       " 'disheveled': {'1': 6, '2': 15},\n",
       " 'time-honored': {'1': 9, '2': 12},\n",
       " 'one-eyed': {'1': 9, '2': 12},\n",
       " 'blood-red': {'1': 12, '2': 8},\n",
       " 'cultured': {'1': 7, '2': 13},\n",
       " 'ill-fated': {'1': 11, '2': 9},\n",
       " 'hackneyed': {'1': 12, '2': 8},\n",
       " 'warped': {'1': 8, '2': 12},\n",
       " 'misdeed': {'1': 12, '2': 8},\n",
       " 'demented': {'1': 6, '2': 14},\n",
       " 'petrified': {'1': 11, '2': 9},\n",
       " 'unattended': {'1': 14, '2': 6},\n",
       " 'sainted': {'1': 15, '2': 5},\n",
       " 'unchallenged': {'1': 7, '2': 13},\n",
       " 'pinched': {'1': 7, '2': 13},\n",
       " 'well-trained': {'1': 9, '2': 10},\n",
       " 'unexplored': {'1': 11, '2': 8},\n",
       " 'curtained': {'1': 12, '2': 7},\n",
       " 'storied': {'1': 9, '2': 10},\n",
       " 'undecided': {'1': 9, '2': 10},\n",
       " 'untrained': {'1': 5, '2': 14},\n",
       " 'unexplained': {'1': 10, '2': 9},\n",
       " 'uninformed': {'1': 11, '2': 7},\n",
       " 'stereotyped': {'1': 11, '2': 7},\n",
       " 'revived': {'1': 10, '2': 8},\n",
       " 'deep-seated': {'1': 7, '2': 11},\n",
       " 'besotted': {'1': 13, '2': 5},\n",
       " 'undeveloped': {'1': 9, '2': 9},\n",
       " 'oiled': {'1': 8, '2': 10},\n",
       " 'barefooted': {'1': 7, '2': 11},\n",
       " 'premeditated': {'1': 12, '2': 6},\n",
       " 'well-defined': {'1': 7, '2': 11},\n",
       " 'unscathed': {'1': 5, '2': 13},\n",
       " 'undeserved': {'1': 9, '2': 9},\n",
       " 'self-satisfied': {'1': 10, '2': 8},\n",
       " 'inflamed': {'1': 8, '2': 10},\n",
       " 'perforated': {'1': 7, '2': 11},\n",
       " 'treasured': {'1': 7, '2': 11},\n",
       " 'modified': {'1': 8, '2': 9},\n",
       " 'jeweled': {'1': 5, '2': 12},\n",
       " 'unaccompanied': {'1': 9, '2': 8},\n",
       " 'cushioned': {'1': 9, '2': 8},\n",
       " 'honeyed': {'1': 9, '2': 8},\n",
       " 'terraced': {'1': 7, '2': 10},\n",
       " 'marbled': {'1': 10, '2': 7},\n",
       " 'well-educated': {'1': 8, '2': 9},\n",
       " 'scorched': {'1': 7, '2': 10},\n",
       " 'debased': {'1': 11, '2': 6},\n",
       " 'single-handed': {'1': 10, '2': 7},\n",
       " 'unabashed': {'1': 8, '2': 9},\n",
       " 'ribbed': {'1': 9, '2': 8},\n",
       " 'undistinguished': {'1': 9, '2': 8},\n",
       " 'naturalized': {'1': 11, '2': 6},\n",
       " 'overloaded': {'1': 7, '2': 9},\n",
       " 'drenched': {'1': 8, '2': 8},\n",
       " 'unrelieved': {'1': 10, '2': 6},\n",
       " 'whitewashed': {'1': 5, '2': 11},\n",
       " 'pickled': {'1': 8, '2': 8},\n",
       " 'able-bodied': {'1': 8, '2': 8},\n",
       " 'crested': {'1': 10, '2': 6},\n",
       " 'glorified': {'1': 7, '2': 9},\n",
       " 'unrecognized': {'1': 6, '2': 9},\n",
       " 'vaunted': {'1': 7, '2': 8},\n",
       " 'allotted': {'1': 10, '2': 5},\n",
       " 'fair-haired': {'1': 8, '2': 7},\n",
       " 'self-imposed': {'1': 9, '2': 6},\n",
       " 'undetermined': {'1': 8, '2': 7},\n",
       " 'feigned': {'1': 10, '2': 5},\n",
       " 'unencumbered': {'1': 6, '2': 9},\n",
       " 'long-legged': {'1': 6, '2': 9},\n",
       " 'wilfred': {'1': 10, '2': 5},\n",
       " 'serried': {'1': 9, '2': 5},\n",
       " 'snow-covered': {'1': 6, '2': 8},\n",
       " 'uncivilized': {'1': 7, '2': 7},\n",
       " 'unimpressed': {'1': 6, '2': 8},\n",
       " 'half-breed': {'1': 8, '2': 6},\n",
       " 'uninvited': {'1': 6, '2': 8},\n",
       " 'rose-colored': {'1': 9, '2': 5},\n",
       " 'well-appointed': {'1': 6, '2': 8},\n",
       " 'maimed': {'1': 9, '2': 5},\n",
       " 'beaded': {'1': 5, '2': 9},\n",
       " 'well-informed': {'1': 6, '2': 8},\n",
       " 'contrived': {'1': 7, '2': 6},\n",
       " 'high-backed': {'1': 6, '2': 7},\n",
       " 'overjoyed': {'1': 8, '2': 5},\n",
       " 'unsatisfied': {'1': 7, '2': 6},\n",
       " 'disembodied': {'1': 5, '2': 7},\n",
       " 'deep-rooted': {'1': 5, '2': 7},\n",
       " 'orphaned': {'1': 5, '2': 7},\n",
       " 'uninspired': {'1': 6, '2': 6},\n",
       " 'darned': {'1': 6, '2': 6},\n",
       " 'caged': {'1': 6, '2': 6},\n",
       " 'heart-shaped': {'1': 5, '2': 7},\n",
       " 'splintered': {'1': 5, '2': 7},\n",
       " 'hard-earned': {'1': 7, '2': 5},\n",
       " 'deranged': {'1': 6, '2': 6},\n",
       " 'strong-minded': {'1': 6, '2': 5},\n",
       " 'oldfashioned': {'1': 5, '2': 6},\n",
       " 'blanched': {'1': 6, '2': 5},\n",
       " 'unimpeded': {'1': 6, '2': 5},\n",
       " 'mystified': {'1': 5, '2': 5},\n",
       " 'tattooed': {'1': 5, '2': 5},\n",
       " 'martyred': {'1': 5, '2': 5},\n",
       " 'belated': {'1': 5, '2': 5},\n",
       " 'distended': {'1': 5, '2': 5},\n",
       " 'inebriated': {'1': 5, '2': 5}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_contexts(per_word_sentences):\n",
    "    for word, groupdata in per_word_sentences.items():\n",
    "        print('word:',word)\n",
    "        print()\n",
    "        for group, data in groupdata.items():\n",
    "            print('group:',group)\n",
    "            print(\"instances of '{}' in corpus {}: {}\".format(word, group, len(data)))\n",
    "            print()\n",
    "            for d in data:\n",
    "                print('identifier:',d['identifier'])\n",
    "                print('context:')\n",
    "                print(d['context'])\n",
    "                print()\n",
    "            print()\n",
    "        print()\n",
    "    print('-----------------')\n",
    "    print()\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_use_dfs(per_word_sentences, target_len=None, ignored_ids=None):\n",
    "    '''\n",
    "    returns {word: dataframe} for each word in input\n",
    "    per_word_sentences: output format of get_sentences\n",
    "    target_len: uses per word per corpus. If target_len is provided: output df has target_len*2 rows unless a corpus has fewer uses of that word,\n",
    "                in which case all are returned. If not target_len: all uses are returned\n",
    "    ignored_ids: specific lines to avoid, e.g., in case of uninterpretable use\n",
    "    '''\n",
    "    \n",
    "    print('Getting dataframes of...')\n",
    "    pws = per_word_sentences.copy()\n",
    "    print(ignored_ids)\n",
    "    if ignored_ids: # for if context inspection revealed any problematic examples\n",
    "        for word, indices in ignored_ids.items():\n",
    "            for group in ['1', '2']:\n",
    "                to_pop = []\n",
    "                for i, data in enumerate(pws[word][group]):\n",
    "                    print(data['identifier'])\n",
    "                    if data['identifier'] in indices: # the identifiers will no longer be consecutive if any are removed,\n",
    "                                                        #but they just need to be unique\n",
    "                        print('found ', data['identifier'])\n",
    "                        to_pop.append(i)\n",
    "                for idx in to_pop:\n",
    "                    pws[word][group].pop(idx)\n",
    "\n",
    "    filtered_uses = {}\n",
    "    for word in pws.keys():\n",
    "        print('\\t', word)\n",
    "        merged_corpora = []\n",
    "        for group in ['1', '2']:\n",
    "            uses = pws[word][group]\n",
    "            corpus_target_len = target_len\n",
    "            if corpus_target_len:\n",
    "                if corpus_target_len > len(uses):\n",
    "                    corpus_target_len = None\n",
    "                    print(\"For '{}', corpus {}: Assigned target length exceeds existing instances. Returning all instances.\".format(word, group))\n",
    "            if corpus_target_len:\n",
    "                shuffle(uses)\n",
    "                uses = uses[:target_len]\n",
    "\n",
    "            print(\"'{}' group {}: {} uses in dataframe.\".format(word, group, len(uses)))\n",
    "            merged_corpora.extend(uses)\n",
    "        filtered_uses[word] = merged_corpora\n",
    "    \n",
    "\n",
    "    use_dfs = {}\n",
    "    for word, data in filtered_uses.items():\n",
    "        use_dfs[word] = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    return use_dfs\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(df_dict, df_type, dirname):\n",
    "    rootdir = os.path.join(os.getcwd(), dirname)\n",
    "    if not os.path.exists(rootdir):\n",
    "        os.mkdir(rootdir)\n",
    "    for word, df in df_dict.items():\n",
    "        worddir = os.path.join(rootdir, word)\n",
    "        if not os.path.exists(worddir):\n",
    "            os.mkdir(worddir)\n",
    "        filepath = os.path.join(worddir, word+'_'+df_type)\n",
    "        df.to_csv(path_or_buf=filepath, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reannotation_update():\n",
    "    '''\n",
    "    updates previous annotations with new annotation data. Adds new judgment and sentences for better interpretability\n",
    "    '''\n",
    "\n",
    "    reannotations = os.listdir('./reannotations')\n",
    "    new_data = []\n",
    "    for word in reannotations:\n",
    "       \n",
    "        if word in os.listdir('./validation_annotations'):\n",
    "            new_annotations = pd.read_csv(os.path.join(os.getcwd(), 'reannotations', word, 'annotations.csv'), sep='\\t')\n",
    "            annotpath = os.path.join(os.getcwd(), 'validation_annotations', word, word+'_instances')\n",
    "            annotations = pd.read_csv(annotpath, sep='\\t')\n",
    "            # DURel shuffles the pairs, so I can't just make a new column with list(new_annotations['judgment'])\n",
    "\n",
    "            identifiers_to_judgment = {frozenset((new_annotations.iloc[i]['identifier1'], new_annotations.iloc[i]['identifier2'])):\n",
    "                             new_annotations.iloc[i]['judgment'] for i in range(len(new_annotations))}\n",
    "            \n",
    "            # These are not necessary for computation, but it allows for the inspection of the usepairs in text-format\n",
    "            # (now that I no longer need to have the DURel format to take into account):\n",
    "            usedf = pd.read_csv(os.path.join(os.getcwd(), 'validation_annotations', word, word+'_uses'), sep='\\t')\n",
    "            id_to_text = {usedf.iloc[i]['identifier']: usedf.iloc[i]['context'] for i in range(len((usedf)))}\n",
    "            new_judgments = []\n",
    "            sentences = []\n",
    "\n",
    "            for i in range(len(annotations)):\n",
    "                ids = frozenset((annotations.iloc[i]['identifier1'], annotations.iloc[i]['identifier2']))\n",
    "                new_judgment = identifiers_to_judgment[ids]\n",
    "                text_format = [id_to_text[identifier] for identifier in list(ids)]\n",
    "                new_judgments.append(new_judgment)\n",
    "                sentences.append(text_format)\n",
    "                \n",
    "            annotations['new_judgments'] = new_judgments\n",
    "            annotations['sentences'] = sentences\n",
    "\n",
    "            annotations.to_csv(path_or_buf=annotpath, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank_correlations():\n",
    "\n",
    "    words = os.listdir('./validation_annotations')\n",
    "\n",
    "    correlations = {'Mean': [], 'Median': []}\n",
    "\n",
    "    combined_means = []\n",
    "    combined_medians = []\n",
    "    combined_new = []\n",
    "    divergence_data = {'combined': {'total':0, 'same':0, 'one': 0, 'more':0}}\n",
    "    divergence_data_pointfives = {'combined': {'total':0, 'same':0, 'one': 0, 'more':0}}\n",
    "\n",
    "\n",
    "    num_point5 = {}\n",
    "    \n",
    "    for word in words:\n",
    "        annotations = pd.read_csv(os.path.join(os.getcwd(), 'validation_annotations', word, word+'_instances'), sep='\\t')\n",
    "        means = list(annotations['avg_judgment'])\n",
    "        medians = list(annotations['median_judgment'])\n",
    "        new = list(annotations['new_judgments'])\n",
    "        \n",
    "        combined_means.extend(means)\n",
    "        combined_medians.extend(medians)\n",
    "        combined_new.extend(new)\n",
    "\n",
    "        num_point5[word] = len([m for m in medians if str(m)[-1] =='5'])\n",
    "\n",
    "        #meanscorr = round(spearmanr(new, means)[0], 2)\n",
    "        #medianscorr = round(spearmanr(new, medians)[0],2)\n",
    "        meanscorr = spearmanr(new, means)[0]\n",
    "        medianscorr = spearmanr(new, medians)[0]\n",
    "        correlations['Mean'].append(meanscorr)\n",
    "        correlations['Median'].append(medianscorr)\n",
    "\n",
    "        divergence_data_pointfives[word] = {'total':0, 'same':0, 'one': 0, 'more':0}\n",
    "        divergence_data[word]= {'total':0, 'same':0, 'one': 0, 'more':0}\n",
    "        \n",
    "        for m, n in zip(medians, new):\n",
    "            pointfive= False\n",
    "            if str(m)[-1] == '5':\n",
    "                pointfive=True\n",
    "                #divergence_data_pointfives[word]['total']+=1\n",
    "                #divergence_data_pointfives['combined']['total']+=1\n",
    "\n",
    "\n",
    "            if m == n:\n",
    "                if not pointfive:\n",
    "                    divergence_data[word]['same']+=1\n",
    "                    divergence_data['combined']['same']+=1\n",
    "                divergence_data_pointfives[word]['same']+=1\n",
    "                divergence_data_pointfives['combined']['same']+=1\n",
    "            elif -1 <= m-n <= 1:\n",
    "                if not pointfive:\n",
    "                    divergence_data[word]['one']+=1\n",
    "                    divergence_data['combined']['one']+=1\n",
    "                divergence_data_pointfives[word]['one']+=1\n",
    "                divergence_data_pointfives['combined']['one']+=1\n",
    "            else:\n",
    "\n",
    "                if not pointfive:\n",
    "                    divergence_data[word]['more']+=1\n",
    "                    divergence_data['combined']['more']+=1\n",
    "                divergence_data_pointfives[word]['more']+=1\n",
    "                divergence_data_pointfives['combined']['more']+=1\n",
    "\n",
    "            if not pointfive:\n",
    "                divergence_data[word]['total']+=1\n",
    "                divergence_data['combined']['total']+=1\n",
    "            divergence_data_pointfives[word]['total']+=1\n",
    "            divergence_data_pointfives['combined']['total']+=1\n",
    "            \n",
    "\n",
    "    # Divergence data was just some post-hoc inclusions and is only briefly mentioned in the paper\n",
    "\n",
    "    for k in divergence_data.keys():\n",
    "        divergence_data[k]['same%'] = divergence_data[k]['same']/divergence_data[k]['total']\n",
    "        divergence_data[k]['one%'] = divergence_data[k]['one']/divergence_data[k]['total']\n",
    "        divergence_data[k]['more%'] = divergence_data[k]['more']/divergence_data[k]['total']\n",
    "\n",
    "        divergence_data_pointfives[k]['same%'] = divergence_data_pointfives[k]['same']/divergence_data_pointfives[k]['total']\n",
    "        divergence_data_pointfives[k]['one%'] = divergence_data_pointfives[k]['one']/divergence_data_pointfives[k]['total']\n",
    "        divergence_data_pointfives[k]['more%'] = divergence_data_pointfives[k]['more']/divergence_data_pointfives[k]['total']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "        #correlations[word] = {}\n",
    "        #correlations[word]['Mean'] = spearmanr(new, means)\n",
    "        #correlations[word]['Median'] = spearmanr(new, medians)\n",
    "\n",
    "    if len(words) > 1:\n",
    "        if len(words) == 2:\n",
    "            comb='both words'\n",
    "        else:\n",
    "            comb='all words'\n",
    "        words.append(comb)\n",
    "        #meanscorr = round(spearmanr(combined_new, combined_means)[0], 2)\n",
    "        #medianscorr = round(spearmanr(combined_new, combined_medians)[0],2)\n",
    "        meanscorr = spearmanr(combined_new, combined_means)[0]\n",
    "        medianscorr = spearmanr(combined_new, combined_medians)[0]\n",
    "        correlations['Mean'].append(meanscorr)\n",
    "        correlations['Median'].append(medianscorr)\n",
    "    \n",
    "        #correlations[comb] = {}\n",
    "        #correlations[comb]['Mean'] = spearmanr(combined_new, combined_means)\n",
    "        #correlations[comb]['Median'] = spearmanr(combined_new, combined_medians)\n",
    "\n",
    "    #for word in words:\n",
    "    #    correlations['Word'].append(word)\n",
    "\n",
    "    print(correlations)\n",
    "\n",
    "\n",
    "    print('both, median', spearmanr(combined_new, combined_medians))\n",
    "    print('both, means', spearmanr(combined_new, combined_means))\n",
    "\n",
    "    print(correlations)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(correlations)\n",
    "    #print(df)\n",
    "    #return df, num_point5\n",
    "\n",
    "    def df_style(styler):\n",
    "        styler.set_caption(\"Correlation of new annotations with existing ones\")\n",
    "        styler.format(precision=3)\n",
    "        styler.format_index(str.upper, axis=1)\n",
    "        styler.relabel_index(words, axis=0)\n",
    "        return styler\n",
    "    \n",
    "\n",
    "    import dataframe_image as dfi\n",
    "\n",
    "    #df = pd.DataFrame(np.random.rand(6,4))\n",
    "    df_styled = df.style \\\n",
    "            .format(precision=3) \\\n",
    "            .format_index(str.upper, axis=1) \\\n",
    "            .relabel_index(words, axis=0) \\\n",
    "            .set_caption(\"Validation correlations\")\n",
    "\n",
    "    dfi.export(df_styled, 'correlations.png',table_conversion = 'matplotlib')\n",
    "    return df, divergence_data, divergence_data_pointfives\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mean': [0.8826923947615539, 0.5553316587662009, 0.8371273389499156], 'Median': [0.8699774895544058, 0.5385252730345806, 0.8258126178146006]}\n",
      "both, median SignificanceResult(statistic=0.8258126178146006, pvalue=1.576019201332923e-13)\n",
      "both, means SignificanceResult(statistic=0.8371273389499156, pvalue=3.6002201300487115e-14)\n",
      "{'Mean': [0.8826923947615539, 0.5553316587662009, 0.8371273389499156], 'Median': [0.8699774895544058, 0.5385252730345806, 0.8258126178146006]}\n"
     ]
    }
   ],
   "source": [
    "df, dd, ddp = get_rank_correlations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.882692</td>\n",
       "      <td>0.869977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555332</td>\n",
       "      <td>0.538525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.837127</td>\n",
       "      <td>0.825813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Mean    Median\n",
       "0  0.882692  0.869977\n",
       "1  0.555332  0.538525\n",
       "2  0.837127  0.825813"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'combined': {'total': 50,\n",
       "  'same': 30,\n",
       "  'one': 20,\n",
       "  'more': 0,\n",
       "  'same%': 0.6,\n",
       "  'one%': 0.4,\n",
       "  'more%': 0.0},\n",
       " 'record_nn': {'total': 25,\n",
       "  'same': 16,\n",
       "  'one': 9,\n",
       "  'more': 0,\n",
       "  'same%': 0.64,\n",
       "  'one%': 0.36,\n",
       "  'more%': 0.0},\n",
       " 'stab_nn': {'total': 25,\n",
       "  'same': 14,\n",
       "  'one': 11,\n",
       "  'more': 0,\n",
       "  'same%': 0.56,\n",
       "  'one%': 0.44,\n",
       "  'more%': 0.0}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'combined': {'total': 36,\n",
       "  'same': 30,\n",
       "  'one': 6,\n",
       "  'more': 0,\n",
       "  'same%': 0.8333333333333334,\n",
       "  'one%': 0.16666666666666666,\n",
       "  'more%': 0.0},\n",
       " 'record_nn': {'total': 19,\n",
       "  'same': 16,\n",
       "  'one': 3,\n",
       "  'more': 0,\n",
       "  'same%': 0.8421052631578947,\n",
       "  'one%': 0.15789473684210525,\n",
       "  'more%': 0.0},\n",
       " 'stab_nn': {'total': 17,\n",
       "  'same': 14,\n",
       "  'one': 3,\n",
       "  'more': 0,\n",
       "  'same%': 0.8235294117647058,\n",
       "  'one%': 0.17647058823529413,\n",
       "  'more%': 0.0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "reannotation_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping attack_nn\n",
      "Skipping gas_nn\n",
      "Skipping lass_nn\n",
      "Skipping word_nn\n",
      "Getting prespecified words. The only specifier filter that will be applied is 'num_usepairs'.\n"
     ]
    }
   ],
   "source": [
    "worddata, skipped_words = get_dwug_data()\n",
    "usepair_dfs, use_dfs = get_uses_and_pairs(worddata, skipped_words=skipped_words, prespecified_words=('record_nn','stab_nn'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(usepair_dfs, 'instances')\n",
    "write_to_file(use_dfs, 'uses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding lemma locations...\n",
      "Retrieving data for...\n",
      "\tCorpus group 1\n",
      "\t\t lame\n",
      "\t\t depressed\n",
      "\t\t crippled\n",
      "\t\t disabled\n",
      "\tCorpus group 2\n",
      "\t\t lame\n",
      "\t\t depressed\n",
      "\t\t crippled\n",
      "\t\t disabled\n",
      "Getting dataframes of...\n",
      "{'lame': ('lame_30', 'lame_3', 'lame_33')}\n",
      "lame_1\n",
      "lame_2\n",
      "lame_3\n",
      "found  lame_3\n",
      "lame_4\n",
      "lame_5\n",
      "lame_6\n",
      "lame_7\n",
      "lame_8\n",
      "lame_9\n",
      "lame_10\n",
      "lame_11\n",
      "lame_12\n",
      "lame_13\n",
      "lame_14\n",
      "lame_15\n",
      "lame_16\n",
      "lame_17\n",
      "lame_18\n",
      "lame_19\n",
      "lame_20\n",
      "lame_21\n",
      "lame_22\n",
      "lame_23\n",
      "lame_24\n",
      "lame_25\n",
      "lame_26\n",
      "lame_27\n",
      "lame_28\n",
      "lame_29\n",
      "lame_30\n",
      "found  lame_30\n",
      "lame_31\n",
      "lame_32\n",
      "lame_33\n",
      "found  lame_33\n",
      "lame_34\n",
      "lame_35\n",
      "\t lame\n",
      "'lame' group 1: 10 uses in dataframe.\n",
      "'lame' group 2: 10 uses in dataframe.\n",
      "\t depressed\n",
      "'depressed' group 1: 10 uses in dataframe.\n",
      "'depressed' group 2: 10 uses in dataframe.\n",
      "\t crippled\n",
      "For 'crippled', corpus 1: Assigned target length exceeds existing instances. Returning all instances.\n",
      "'crippled' group 1: 8 uses in dataframe.\n",
      "For 'crippled', corpus 2: Assigned target length exceeds existing instances. Returning all instances.\n",
      "'crippled' group 2: 8 uses in dataframe.\n",
      "\t disabled\n",
      "'disabled' group 1: 10 uses in dataframe.\n",
      "'disabled' group 2: 10 uses in dataframe.\n"
     ]
    }
   ],
   "source": [
    "candidates = {'disabled': {'forms': ('disabled',), 'tags': ('JJ', 'JJR', 'JJS')},\n",
    "              'crippled': {'forms': ('crippled',), 'tags': ('JJ', 'JJR', 'JJS')},\n",
    "              'depressed': {'forms': ('depressed',), 'tags': ('JJ', 'JJR', 'JJS')},\n",
    "              'lame': {'forms': ('lame', 'lamer', 'lamest'), 'tags': ('JJ', 'JJR', 'JJS')},\n",
    "              'sentence': {'forms': ('sentence', 'sentences'), 'tags': ('NN', 'NNS')},\n",
    "              'meaning': {'forms': ('meaning', 'meanings'), 'tags': ('NN', 'NNS')},\n",
    "              'hit': {'forms': ('hit', 'hits', 'hitting'), 'tags': ('VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ')},\n",
    "              'strike_nn': {'forms': ('strike', 'strikes'), 'tags': ('NN', 'NNS'), 'search_lemma': 'strike'},\n",
    "              'stroke_nn': {'forms': ('stroke', 'strokes'), 'tags': ('NN', 'NNS'), 'search_lemma': 'stroke'},\n",
    "              'strike_vb': {'forms': ('strike', 'strikes','struck', 'striking'), 'tags': ('VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'), 'search_lemma': 'strike'},\n",
    "              #'hit': {'forms': ('hit',), 'tags': ('NN', 'NNS')},\n",
    "              'meter': {'forms': ('meter', 'metre', 'meters', 'metres'), 'tags': ('NN', 'NNS')},\n",
    "              'hand': {'forms': ('hand', 'hands'), 'tags': ('NN', 'NNS')}, # head as validationautomation \n",
    "              'automation': {'forms': ('automation',), 'tags': ('NN', 'NNS')}, # not really NNS (and it likely does not occur), but shouldn't catch unsought meanings             \n",
    "\n",
    "              }\n",
    "candidates2 = {'lame': {'forms': ('lame', 'lamer', 'lamest'), 'tags': ('JJ', 'JJR', 'JJS')},\n",
    "               'depressed': {'forms': ('depressed',), 'tags': ('JJ', 'JJR', 'JJS')},\n",
    "               'crippled': {'forms': ('crippled',), 'tags': ('JJ', 'JJR', 'JJS')},\n",
    "               'disabled': {'forms': ('disabled',), 'tags': ('JJ', 'JJR', 'JJS')},\n",
    "               }\n",
    "\n",
    "per_word_sentences = get_sentences(candidates2)\n",
    "ccoha_use_dfs = get_use_dfs(per_word_sentences, ignored_ids={'lame': ('lame_30','lame_3', 'lame_33' ), }, target_len=10) #ignored_ids={'disabled':(1,2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(ccoha_use_dfs, 'uses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    worddata, skipped_words = get_dwug_data()#target_words=('stab_nn', 'savage_nn'), target_pos=('vb')\n",
    "    usepair_dfs, use_dfs = get_uses_and_pairs(worddata, skipped_words=skipped_words, prespecified_words=('word_nn','stab_nn'))\n",
    "\n",
    "    candidates = {'disabled': {'forms': ('disabled',), 'tags': ('JJ', 'JJR', 'JJS')},\n",
    "              'crippled': {'forms': ('crippled',), 'tags': ('JJ', 'JJR', 'JJS')},\n",
    "              'depressed': {'forms': ('depressed',), 'tags': ('JJ', 'JJR', 'JJS')},\n",
    "              'lame': {'forms': ('lame', 'lamer', 'lamest'), 'tags': ('JJ', 'JJR', 'JJS')}\n",
    "              }\n",
    "    per_word_sentences = get_sentences(candidates)\n",
    "    ccoha_use_dfs = get_use_dfs(per_word_sentences, target_len=10) #ignored_ids={'disabled':(1,2,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
